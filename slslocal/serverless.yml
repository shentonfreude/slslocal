service: slslocal

# You can pin your service to only deploy with a specific Serverless version
# Check out our docs for more details
# frameworkVersion: "=X.X.X"
frameworkVersion: "=1.26.0"

provider:
  name: aws
  runtime: python3.6
  # defaults: stage=dev region=us-east-1
  stage: dev
  region: us-east-1

# you can add statements to the Lambda function's IAM Role here
#  iamRoleStatements:
#    - Effect: "Allow"
#      Action:
#        - "s3:ListBucket"
#      Resource: { "Fn::Join" : ["", ["arn:aws:s3:::", { "Ref" : "ServerlessDeploymentBucket" } ] ]  }
#    - Effect: "Allow"
#      Action:
#        - "s3:PutObject"
#      Resource:
#        Fn::Join:
#          - ""
#          - - "arn:aws:s3:::"
#            - "Ref" : "ServerlessDeploymentBucket"
#            - "/*"

plugins:
  - serverless-localstack

custom:
  localstack:
    host: http://localhost
    # Only use localstack plugin for stages: local
    stages:
      - local
      #- dev
    endpoints:          # sorted by port
      Kinesis:          http://localhost:4568
      DynamoDB:         http://localhost:4570
      Elasticsearch:    http://localhost:4571
      S3:               http://localhost:4572
      Lambda:           http://localhost:4574
      SNS:              http://localhost:4575
      SQS:              http://localhost:4576
      ES:               http://localhost:4578
      CloudFormation:   http://localhost:4581
      # Starting mock API Gateway (http port 4567)...
      # Starting mock DynamoDB Streams service (http port 4570)...
      # Starting mock Firehose service (http port 4573)...
      # Starting mock Redshift (http port 4577)...
      # Starting mock SES (http port 4579)...
      # Starting mock CloudWatch (http port 4582)...
      # Starting mock SSM (http port 4583)...


# you can define service wide environment variables here
#  environment:
#    variable1: value1

# you can add packaging information here
package:
#  include:
#    - include-me.py
#    - include-me-dir/**
 exclude:
   - node_modules/**

functions:
  hello:
    handler: handler.hello

  gets3metadata:
    handler: handler.gets3metadata
    events:
      - s3:
          # Let SLS create the bucket, have to use its naming convention
          bucket: chrisslslocal
          event: s3:ObjectCreated:*
          # rules:
          #   - prefix: in/upload/image/
          #   - suffix: .jpg


#      - stream: arn:aws:dynamodb:region:XXXXXX:table/foo/stream/1970-01-01T00:00:00.000

#    Define function environment variables here
#    environment:
#      variable2: value2

# Getting errors from localstack after deployment bucket created
# so lets just comment them out for now
# Resources:
#   S3BucketChrisslslocal:        # name mangled from s3.bucket=chrisslslocal above
#     Type: AWS::S3::Bucket
#     Properties:
#       BucketName: chris-slslocal
#     Gets3metadataLambdaPermissionChrisslslocalS3:
#       Type: "AWS::Lambda::Permission"
#       Properties:
#         FunctionName:
#           "Fn::GetAtt":
#             - Gets3metadataLambdaFunction
#             - Arn
#         Principal: "s3.amazonaws.com"
#         Action: "lambda:InvokeFunction"
#         SourceAccount:
#           Ref: AWS::AccountId
#         SourceArn: "arn:aws:s3:::chris-slslocal"
